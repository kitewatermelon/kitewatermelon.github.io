---
title: "[논문리뷰] Supervised Contrastive Learning"
last_modified_at: 2026-01-12
categories:
  - 논문리뷰
tags:
  - Computer Vision
  - Supervised Contrastive Learning 
  - NIPS
excerpt: "Supervised Contrastive Learning (NIPS 2020)"
use_math: true
classes: wide
---
> NIPS 2020 [[Paper](https://arxiv.org/pdf/2004.11362)] [[Github](https://github.com/google-research/google-research/tree/master/supcon)]  
> Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan   
> 23 Apr 2020  

## Introduction


Cross Entropy(CE)는 딥러닝 분류기의 지도 학습에서 가장 널리 사용되는 loss이다. CE의 문제는 여러 가지가 보고되어 왔는데 저자는 세가지를 언급하며 서론을 시작한다.

---
1. lack of robustness to noisy labels (노이즈에 약하고, 모델이 잘못된 라벨을 의심하지 않고 그대로 외운다.)
2. possibility of poor margins (decision boundary가 샘플에 너무 바짝 붙어 있다.)
3. leading to reduced generalization performance (학습에 쓰지 않은 데이터에서 잘 맞추는 능력이 떨어진다.)
---

그럼에도 불구하고 위 세가지 단점을 커버하는 대부분의 loss는 Large scale dataset(e.g. ImageNet)에서 작동하지 않아서 CE를 계속 사용하게 된다.

최근 몇년간 Contrastive Learning의 부활은 Self-Supervised Learning(SSL)의 눈부신 발전을 이끌었다.Contrastive Learning의 아이디어는 embedding space에서 anchor를 두고 "positive" sample은 당기고 "negative" sample은 밀어버리자는 것이다. SSL에서는 레이블이 없기 때문에 같은 이미지의 다른 view만이 (augmentation을 통해 다른 view를 만든다.) positive sample이 된다. negative sample은 mibibatch에서 랜덤하게 샘플링한다.

<center><img src='{​{"/assets/img/supcon/fig2.webp" | relative_url}}' width="20%"></center>
<br>

이 논문에서는 







## Limitation
  
## 개인적인 생각
